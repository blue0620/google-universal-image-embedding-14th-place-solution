{"cells":[{"cell_type":"markdown","metadata":{},"source":["This inference notebook is based on [motono0223's notebook](https://www.kaggle.com/code/motono0223/guie-clip-tensorflow-train-example) and his datasets with some improvements.\n","\n","\n","## Model\n","### for training:\n","- STEP1 Training without backbone model layers\n","\n","backbone(CLIP VIT with [openai/clip-vit-large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336)) + Dropout + Dense(units=64) + Arcface + Softmax (classes=17888)\n","\n","Dataset of STEP1:\n","- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n","  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n","  To reduce the dataset size, this dataset has only 50 images per class.  \n","\n","- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n","  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n","  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n","\n","- STEP2 Training with all model layers\n","\n","backbone(CLIP VIT with [pretrained clip-vit-large-patch14-336(STEP1 model)](https://huggingface.co/openai/clip-vit-large-patch14-336)) + Dropout + Dense(units=64) + Arcface + Softmax (classes=17888)\n","\n","- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n","- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)\n","- [stanford cars]()\n","\n","- STEP3 Training with all model layers ()\n","\n","backbone(CLIP VIT with [pretrained clip-vit-large-patch14-336(STEP2 model)](https://huggingface.co/openai/clip-vit-large-patch14-336)) + Dropout + Dense(units=64) + Arcface + Softmax (classes=17888)\n","\n","- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n","- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)\n","- [stanford cars]()\n","- [imagenet-mini with animal classes]()\n","- [MET Artwork Dataset with 9 images per class]()\n","\n","\n","- for inference:  \n","\n","backbone(CLIP) + Dropout + Dense(units=64) + AdaptiveAveragePooling(n=64)\n"]},{"cell_type":"markdown","metadata":{"id":"cUF4H1xBsYb6"},"source":["# Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:45:45.326306Z","iopub.status.busy":"2022-12-03T07:45:45.325923Z","iopub.status.idle":"2022-12-03T07:45:45.359557Z","shell.execute_reply":"2022-12-03T07:45:45.358406Z","shell.execute_reply.started":"2022-12-03T07:45:45.326227Z"},"id":"05oZ1Q5Idhj-","outputId":"548ec03d-91de-4a68-bfb9-9830ea763b5e","trusted":true},"outputs":[],"source":["import os\n","def is_colab_env():\n","    is_colab = False\n","    for k in os.environ.keys():\n","        if \"COLAB\" in k:\n","            is_colab = True\n","            break\n","    return is_colab\n","\n","# if google colab, install transformers and tensorflow_addons\n","# (Note: please use google colab(TPU) when model is trained. \n","#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\n","if is_colab_env():\n","    !pip install transformers\n","    !pip install tensorflow_addons"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:45:45.362673Z","iopub.status.busy":"2022-12-03T07:45:45.361932Z","iopub.status.idle":"2022-12-03T07:45:59.963793Z","shell.execute_reply":"2022-12-03T07:45:59.962792Z","shell.execute_reply.started":"2022-12-03T07:45:45.362632Z"},"id":"i72153AaDJds","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-03 07:45:54.174577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:54.175628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:54.176331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:54.178285: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-03 07:45:54.178599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:54.179341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:54.180089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:59.109891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:59.110797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:59.111519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-12-03 07:45:59.112182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]}],"source":["from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n","\n","import re\n","import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import metrics\n","from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n","from tensorflow.keras import backend as K\n","import tensorflow_addons as tfa\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import normalize\n","import pickle\n","import json\n","import tensorflow_hub as tfhub\n","from datetime import datetime\n","import gc\n","import requests\n","from mpl_toolkits import axes_grid1"]},{"cell_type":"markdown","metadata":{"id":"k8BdwSO1sYcN"},"source":["# Device"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:45:59.966721Z","iopub.status.busy":"2022-12-03T07:45:59.965725Z","iopub.status.idle":"2022-12-03T07:45:59.977852Z","shell.execute_reply":"2022-12-03T07:45:59.975670Z","shell.execute_reply.started":"2022-12-03T07:45:59.966681Z"},"id":"khrTPhLcR39a","outputId":"7adb1df9-4f27-4042-c084-cfc7881b8728","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["REPLICAS:  1\n"]}],"source":["import tensorflow as tf\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","else:\n","    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","    strategy = tf.distribute.get_strategy()\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:45:59.980029Z","iopub.status.busy":"2022-12-03T07:45:59.979509Z","iopub.status.idle":"2022-12-03T07:45:59.996901Z","shell.execute_reply":"2022-12-03T07:45:59.995792Z","shell.execute_reply.started":"2022-12-03T07:45:59.979994Z"},"id":"tKetdL8BsYcQ","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-03 07:45:59.988163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"]}],"source":["# If GPU instance, it makes mixed precision enable.\n","if strategy.num_replicas_in_sync == 1:\n","    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n","    policy = mixed_precision.Policy('mixed_float16')\n","    mixed_precision.set_policy(policy) "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:46:00.001133Z","iopub.status.busy":"2022-12-03T07:46:00.000723Z","iopub.status.idle":"2022-12-03T07:46:00.011082Z","shell.execute_reply":"2022-12-03T07:46:00.010024Z","shell.execute_reply.started":"2022-12-03T07:46:00.001102Z"},"id":"lCwQB_L1NGoH","outputId":"040dbaa6-54e6-4135-8251-bd779314b05f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["clip-vit-large-patch14-336\n"]}],"source":["class config:\n","    VERSION = 3\n","    SUBV = \"Clip_ViT_Train\"\n","\n","    SEED = 42\n","\n","    # pretrained model\n","    RESUME = True\n","    RESUME_EPOCH = 0\n","    RESUME_WEIGHT = \"./input/final-clip-vit-l-patch14-336pix-emb64-unf/iandmet-normal-clip-vit-large-patch14_336pix-emb64_loss_01.h5\"\n","\n","    # backbone model\n","    model_type = \"clip-vit-large-patch14-336\"\n","    EFF_SIZE = 0\n","    EFF2_TYPE = \"\"\n","    IMAGE_SIZE = 336\n","\n","    # projection layer\n","    N_CLASSES = 17888\n","    EMB_DIM = 64  # = 64 x N\n","    \n","    # training\n","    TRAIN = False\n","    BATCH_SIZE = 200 * strategy.num_replicas_in_sync\n","    EPOCHS = 100\n","    LR = 0.001\n","    save_dir = \"./\"\n","\n","    DEBUG = False\n","    \n","\n","# Function to seed everything\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    \n","# model name\n","MODEL_NAME = None\n","if config.model_type == 'effnetv1':\n","    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n","elif config.model_type == 'effnetv2':\n","    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n","elif \"swin\" in config.model_type:\n","    MODEL_NAME = config.model_type\n","elif \"conv\" in config.model_type:\n","    MODEL_NAME = config.model_type\n","else:\n","    MODEL_NAME = config.model_type\n","config.MODEL_NAME = MODEL_NAME\n","print(MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"aZuGi10XOuiW"},"source":["# Dataset pipeline"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:46:17.398858Z","iopub.status.busy":"2022-12-03T07:46:17.398308Z","iopub.status.idle":"2022-12-03T07:46:17.410384Z","shell.execute_reply":"2022-12-03T07:46:17.409163Z","shell.execute_reply.started":"2022-12-03T07:46:17.398817Z"},"id":"UDsIA9z0NXIe","trusted":true},"outputs":[],"source":["def arcface_format(image, label_group):\n","    return {'inp1': image, 'inp2': label_group}, label_group\n","\n","def rescale_image(image, label_group):\n","    image = tf.cast(image, tf.float32) * 255.0\n","    return image, label_group\n","\n","# Data augmentation function\n","def data_augment(image, label_group):\n","    image = tf.image.random_flip_left_right(image)\n","    #image = tf.image.random_flip_up_down(image)\n","    image = tf.image.random_hue(image, 0.01)\n","    image = tf.image.random_saturation(image, 0.70, 1.30)\n","    image = tf.image.random_contrast(image, 0.80, 1.20)\n","    image = tf.image.random_brightness(image, 0.10)\n","    return image, label_group\n","\n","# Dataset to obtain backbone's inference\n","def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n","    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n","    data_len = sum( [ get_num_of_image(file) for file in tfrecord_paths ] )\n","    dataset = dataset.shuffle( data_len//10 ) if shuffle else dataset\n","    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n","    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n","\n","    if augment:\n","        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n","    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n","    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n","    if repeat:\n","        dataset = dataset.repeat()\n","    dataset = dataset.batch(config.BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO)\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"dOPX4LshNXsM"},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:46:17.413704Z","iopub.status.busy":"2022-12-03T07:46:17.413066Z","iopub.status.idle":"2022-12-03T07:46:17.431462Z","shell.execute_reply":"2022-12-03T07:46:17.430616Z","shell.execute_reply.started":"2022-12-03T07:46:17.413658Z"},"id":"1LjaDRLgMjdq","trusted":true},"outputs":[],"source":["# Arcmarginproduct class keras layer\n","class ArcMarginProduct(tf.keras.layers.Layer):\n","    '''\n","    Implements large margin arc distance.\n","\n","    Reference:\n","        https://arxiv.org/pdf/1801.07698.pdf\n","        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n","            blob/master/src/modeling/metric_learning.py\n","    '''\n","    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n","                 ls_eps=0.0, **kwargs):\n","\n","        super(ArcMarginProduct, self).__init__(**kwargs)\n","\n","        self.n_classes = n_classes\n","        self.s = s\n","        self.m = m\n","        self.ls_eps = ls_eps\n","        self.easy_margin = easy_margin\n","        self.cos_m = tf.math.cos(m)\n","        self.sin_m = tf.math.sin(m)\n","        self.th = tf.math.cos(math.pi - m)\n","        self.mm = tf.math.sin(math.pi - m) * m\n","\n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'n_classes': self.n_classes,\n","            's': self.s,\n","            'm': self.m,\n","            'ls_eps': self.ls_eps,\n","            'easy_margin': self.easy_margin,\n","        })\n","        return config\n","\n","    def build(self, input_shape):\n","        super(ArcMarginProduct, self).build(input_shape[0])\n","\n","        self.W = self.add_weight(\n","            name='W',\n","            shape=(int(input_shape[0][-1]), self.n_classes),\n","            initializer='glorot_uniform',\n","            dtype='float32',\n","            trainable=True,\n","            regularizer=None)\n","\n","    def call(self, inputs):\n","        X, y = inputs\n","        y = tf.cast(y, dtype=tf.int32)\n","        cosine = tf.matmul(\n","            tf.math.l2_normalize(X, axis=1),\n","            tf.math.l2_normalize(self.W, axis=0)\n","        )\n","        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        if self.easy_margin:\n","            phi = tf.where(cosine > 0, phi, cosine)\n","        else:\n","            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n","        one_hot = tf.cast(\n","            tf.one_hot(y, depth=self.n_classes),\n","            dtype=cosine.dtype\n","        )\n","        if self.ls_eps > 0:\n","            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n","\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n","        output *= self.s\n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:46:17.435476Z","iopub.status.busy":"2022-12-03T07:46:17.433423Z","iopub.status.idle":"2022-12-03T07:46:17.453748Z","shell.execute_reply":"2022-12-03T07:46:17.452701Z","shell.execute_reply.started":"2022-12-03T07:46:17.435438Z"},"id":"jX76WJYoMgey","trusted":true},"outputs":[],"source":["def get_scale_layer(rescale_mode = \"tf\"):\n","    # For keras_cv_attention_models module:\n","    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n","    # ref function : init_mean_std_by_rescale_mode()\n","\n","    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n","\n","    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n","        mean, std = rescale_mode\n","    elif rescale_mode == \"torch\":\n","        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n","        std = np.array([0.229, 0.224, 0.225]) * 255.0\n","    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n","        mean, std = 127.5, 127.5\n","    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n","        mean, std = 128.0, 128.0\n","    elif rescale_mode == \"raw01\":\n","        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n","    else:\n","        mean, std = 0, 1  # raw inputs [0, 255]        \n","    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n","    \n","    return scaling_layer\n","\n","\n","def get_clip_model():\n","    inp = tf.keras.layers.Input(shape = [3, 336, 336]) # [B, C, H, W]\n","    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\",from_pt=True)\n","    output = backbone({'pixel_values':inp}).pooler_output\n","    return tf.keras.Model(inputs=[inp], outputs=[output])\n","\n","def get_embedding_model():\n","    #------------------\n","    # Definition of placeholders\n","    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n","    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n","\n","    # Definition of layers\n","    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n","    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n","    layer_permute = tf.keras.layers.Permute((3,1,2))\n","    layer_backbone = get_clip_model()\n","    layer_dropout = tf.keras.layers.Dropout(0.2)\n","    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM)\n","    layer_margin = ArcMarginProduct(\n","        n_classes = config.N_CLASSES, \n","        s = 30, \n","        m = 0.3, \n","        name=f'head/arcface', \n","        dtype='float32'\n","        )\n","    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n","    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n","    \n","    if config.EMB_DIM != 64:\n","        layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n","    else:\n","        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x )  # layer with no operation\n","\n","    #------------------\n","    # Definition of entire model\n","    image = layer_scaling(inp)\n","    image = layer_resize(image)\n","    image = layer_permute(image)\n","    backbone_output = layer_backbone(image)\n","    embed = layer_dropout(backbone_output)\n","    embed = layer_dense_before_arcface(embed)\n","    x = layer_margin([embed, label])\n","    output = layer_softmax(x)\n","    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n","\n","    model.layers[-6].trainable = False\n","    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n","    model.compile(\n","        optimizer = opt,\n","        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n","        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n","        )\n","\n","    #------------------\n","    # Definition of embedding model (for submission)\n","    embed_model = keras.Sequential([\n","        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n","        layer_scaling,\n","        layer_resize,\n","        layer_permute,\n","        layer_backbone,\n","        layer_dropout,\n","        layer_dense_before_arcface,\n","        layer_adaptive_pooling,    # shape:[None, config.EMB_DIM] --> [None, 64]\n","        layer_l2,\n","    ])\n","\n","\n","    return model, embed_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:46:17.457346Z","iopub.status.busy":"2022-12-03T07:46:17.457037Z","iopub.status.idle":"2022-12-03T07:48:08.744514Z","shell.execute_reply":"2022-12-03T07:48:08.743096Z","shell.execute_reply.started":"2022-12-03T07:46:17.457317Z"},"id":"5nGM8XncMglt","outputId":"a8526809-ce45-4b10-f642-a72c6014cb1e","trusted":true},"outputs":[],"source":["with strategy.scope():\n","    model, emb_model = get_embedding_model()\n","\n","if config.RESUME:\n","    print(f\"load {config.RESUME_WEIGHT}\")\n","    model.load_weights( config.RESUME_WEIGHT )\n","    #emb_model.load_weights( config.RESUME_WEIGHT )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:48:08.747836Z","iopub.status.busy":"2022-12-03T07:48:08.747070Z","iopub.status.idle":"2022-12-03T07:48:08.775588Z","shell.execute_reply":"2022-12-03T07:48:08.774536Z","shell.execute_reply.started":"2022-12-03T07:48:08.747797Z"},"id":"qSWkPesHMgpO","outputId":"b65d4347-69b9-4d35-fcc4-02cf8e1d68b7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","inp1 (InputLayer)               [(None, None, None,  0                                            \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, None, None, 3 0           inp1[0][0]                       \n","__________________________________________________________________________________________________\n","resize (Lambda)                 (None, 336, 336, 3)  0           lambda[0][0]                     \n","__________________________________________________________________________________________________\n","permute (Permute)               (None, 3, 336, 336)  0           resize[0][0]                     \n","__________________________________________________________________________________________________\n","model (Functional)              (None, 1024)         303507456   permute[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_24 (Dropout)            (None, 1024)         0           model[0][0]                      \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 64)           65600       dropout_24[0][0]                 \n","__________________________________________________________________________________________________\n","inp2 (InputLayer)               [(None,)]            0                                            \n","__________________________________________________________________________________________________\n","head/arcface (ArcMarginProduct) (None, 17888)        1144832     dense[0][0]                      \n","                                                                 inp2[0][0]                       \n","__________________________________________________________________________________________________\n","softmax (Softmax)               (None, 17888)        0           head/arcface[0][0]               \n","==================================================================================================\n","Total params: 304,717,888\n","Trainable params: 1,210,432\n","Non-trainable params: 303,507,456\n","__________________________________________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:48:08.777694Z","iopub.status.busy":"2022-12-03T07:48:08.776850Z","iopub.status.idle":"2022-12-03T07:48:08.802752Z","shell.execute_reply":"2022-12-03T07:48:08.801643Z","shell.execute_reply.started":"2022-12-03T07:48:08.777658Z"},"id":"0a5LW0tnMgsX","outputId":"9143889a-8d13-493d-bb8a-f0323a7da97f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lambda (Lambda)              (None, None, None, 3)     0         \n","_________________________________________________________________\n","resize (Lambda)              (None, 336, 336, 3)       0         \n","_________________________________________________________________\n","permute (Permute)            (None, 3, 336, 336)       0         \n","_________________________________________________________________\n","model (Functional)           (None, 1024)              303507456 \n","_________________________________________________________________\n","dropout_24 (Dropout)         (None, 1024)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                65600     \n","_________________________________________________________________\n","lambda_1 (Lambda)            (None, 64)                0         \n","_________________________________________________________________\n","embedding_norm (Lambda)      (None, 64)                0         \n","=================================================================\n","Total params: 303,573,056\n","Trainable params: 65,600\n","Non-trainable params: 303,507,456\n","_________________________________________________________________\n"]}],"source":["emb_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"5z_2vp1TsYcx"},"source":["# Scheduler"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:48:08.804687Z","iopub.status.busy":"2022-12-03T07:48:08.804342Z","iopub.status.idle":"2022-12-03T07:48:11.143073Z","shell.execute_reply":"2022-12-03T07:48:11.139491Z","shell.execute_reply.started":"2022-12-03T07:48:08.804653Z"},"trusted":true},"outputs":[],"source":["# save for debug\n","emb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )"]},{"cell_type":"markdown","metadata":{"id":"kG-r1le7SF9F"},"source":["# Create submission.zip"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-03T07:48:11.167544Z","iopub.status.busy":"2022-12-03T07:48:11.158087Z","iopub.status.idle":"2022-12-03T07:49:56.183454Z","shell.execute_reply":"2022-12-03T07:49:56.182238Z","shell.execute_reply.started":"2022-12-03T07:48:11.167471Z"},"id":"_eeqo14RMxEr","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-12-03 07:48:40.524837: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"]}],"source":["save_locally = tf.saved_model.SaveOptions(\n","    experimental_io_device='/job:localhost'\n",")\n","emb_model.save('./embedding_norm_model', options=save_locally)\n","\n","from zipfile import ZipFile\n","\n","with ZipFile('submission.zip','w') as zip:           \n","    zip.write(\n","        './embedding_norm_model/saved_model.pb', \n","        arcname='saved_model.pb'\n","    ) \n","    zip.write(\n","        './embedding_norm_model/variables/variables.data-00000-of-00001', \n","        arcname='variables/variables.data-00000-of-00001'\n","    ) \n","    zip.write(\n","        './embedding_norm_model/variables/variables.index', \n","        arcname='variables/variables.index'\n","    )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
